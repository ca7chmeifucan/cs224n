{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load run.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.treebank import StanfordSentiment\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from word2vec import *\n",
    "from sgd import *\n",
    "\n",
    "# Check Python Version\n",
    "import sys\n",
    "assert sys.version_info[0] == 3\n",
    "assert sys.version_info[1] >= 5\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "\n",
    "startTime=time.time()\n",
    "wordVectors = np.concatenate(\n",
    "    ((np.random.rand(nWords, dimVectors) - 0.5) /\n",
    "       dimVectors, np.zeros((nWords, dimVectors))),\n",
    "    axis=0)\n",
    "wordVectors = sgd(\n",
    "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n",
    "        negSamplingLossAndGradient),\n",
    "    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n",
    "# Note that normalization is not called here. This is not a bug,\n",
    "# normalizing during training loses the notion of length.\n",
    "\n",
    "print(\"sanity check: cost at convergence should be around or below 10\")\n",
    "print(\"training took %d seconds\" % (time.time() - startTime))\n",
    "\n",
    "# concatenate the input and output word vectors\n",
    "wordVectors = np.concatenate(\n",
    "    (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
    "    axis=0)\n",
    "\n",
    "visualizeWords = [\n",
    "    \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "    \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"dumb\",\n",
    "    \"annoying\", \"female\", \"male\", \"queen\", \"king\", \"man\", \"woman\", \"rain\", \"snow\",\n",
    "    \"hail\", \"coffee\", \"tea\"]\n",
    "\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2])\n",
    "\n",
    "for i in range(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i],\n",
    "        bbox=dict(facecolor='green', alpha=0.1))\n",
    "\n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n",
    "\n",
    "plt.savefig('word_vectors.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = (np.random.rand(10, 5) - 0.5)/5\n",
    "b = np.zeros((10, 5))\n",
    "# np.concatenate((a, b), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0798436 ,  0.05805339,  0.03524716,  0.05761358,  0.06014616],\n",
       "       [-0.01721903,  0.06644677, -0.02871379,  0.08672382,  0.05593952],\n",
       "       [ 0.08984317,  0.03692106,  0.04009998, -0.08315423,  0.08274837],\n",
       "       [-0.03797086, -0.0803641 , -0.07577984,  0.02143174,  0.09954647],\n",
       "       [ 0.06564412, -0.03425285, -0.05248864,  0.0654679 , -0.04514957],\n",
       "       [ 0.08486709, -0.00790751,  0.06081152, -0.02059584,  0.07834886],\n",
       "       [ 0.01560422, -0.03299115,  0.04701667,  0.0404169 ,  0.05139907],\n",
       "       [ 0.03958126, -0.08606887, -0.05475353,  0.07723591,  0.02461397],\n",
       "       [ 0.09863331, -0.02517874, -0.0488132 ,  0.06040181,  0.00122362],\n",
       "       [-0.04865539,  0.08740711, -0.03472863, -0.02851087, -0.08709646],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.concatenate((a, b), axis=0)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0798436 ,  0.05805339,  0.03524716,  0.05761358,  0.06014616],\n",
       "       [-0.01721903,  0.06644677, -0.02871379,  0.08672382,  0.05593952],\n",
       "       [ 0.08984317,  0.03692106,  0.04009998, -0.08315423,  0.08274837],\n",
       "       [-0.03797086, -0.0803641 , -0.07577984,  0.02143174,  0.09954647],\n",
       "       [ 0.06564412, -0.03425285, -0.05248864,  0.0654679 , -0.04514957],\n",
       "       [ 0.08486709, -0.00790751,  0.06081152, -0.02059584,  0.07834886],\n",
       "       [ 0.01560422, -0.03299115,  0.04701667,  0.0404169 ,  0.05139907],\n",
       "       [ 0.03958126, -0.08606887, -0.05475353,  0.07723591,  0.02461397],\n",
       "       [ 0.09863331, -0.02517874, -0.0488132 ,  0.06040181,  0.00122362],\n",
       "       [-0.04865539,  0.08740711, -0.03472863, -0.02851087, -0.08709646],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((c[:10, :], c[10:, :]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
